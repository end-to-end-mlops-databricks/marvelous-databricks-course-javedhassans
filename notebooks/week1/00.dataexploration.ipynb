{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ActigraphAggregation:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_data(self, participant_id):\n",
    "        file_path = os.path.join(self.root_dir, f\"id={participant_id}\", \"part-0.parquet\")\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data[\"id\"] = participant_id\n",
    "        print(f\"Loaded data columns for {participant_id}: {data.columns.tolist()}\")\n",
    "        return data\n",
    "\n",
    "    def aggregate_actigraphy(self, data):\n",
    "        \"\"\"\n",
    "        Aggregate the actigraphy data for a single participant.\n",
    "\n",
    "        Parameters:\n",
    "        - data (DataFrame): Actigraphy data for a participant.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Aggregated actigraphy data with one row per 'id'.\n",
    "        \"\"\"\n",
    "        aggregated_df = (\n",
    "            data.groupby(\"id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"X\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Y\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Z\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"enmo\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"anglez\": \"mean\",\n",
    "                    \"non-wear_flag\": \"sum\",\n",
    "                    \"light\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"battery_voltage\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Flatten column names\n",
    "        aggregated_df.columns = [\"_\".join(col).strip() if col[1] else col[0] for col in aggregated_df.columns.values]\n",
    "        return aggregated_df\n",
    "\n",
    "    def process_participant_data(self, participant_id):\n",
    "        data = self.load_data(participant_id)\n",
    "        aggregated_data = self.aggregate_actigraphy(data)\n",
    "        return aggregated_data\n",
    "\n",
    "    def process_all_participants(self):\n",
    "        all_data = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for id_folder in os.listdir(self.root_dir):\n",
    "                if not id_folder.startswith(\"id=\"):\n",
    "                    continue\n",
    "                participant_id = id_folder.split(\"=\")[-1]\n",
    "                futures.append(executor.submit(self.process_participant_data, participant_id))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing participants\"):\n",
    "                participant_data = future.result()\n",
    "                all_data.append(participant_data)\n",
    "\n",
    "        feature_table = pd.concat(all_data, ignore_index=True)\n",
    "        return feature_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the root directory containing participant data\n",
    "aggregator = ActigraphAggregation(root_dir=\"../../data/series_test.parquet/\")\n",
    "\n",
    "# Process all participants and get the final aggregated feature table\n",
    "feature_table = aggregator.process_all_participants()\n",
    "\n",
    "# Inspect the feature table\n",
    "print(feature_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ActigraphAggregation:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_data(self, participant_id):\n",
    "        file_path = os.path.join(self.root_dir, f\"id={participant_id}\", \"part-0.parquet\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found for participant {participant_id}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if file doesn't exist\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data[\"id\"] = participant_id\n",
    "        print(f\"Loaded data columns for {participant_id}: {data.columns.tolist()}\")\n",
    "        return data\n",
    "\n",
    "    def temporal_aggregations(self, data):\n",
    "        # Classify weekday/weekend\n",
    "        data[\"weekday_flag\"] = data[\"weekday\"].apply(lambda x: \"weekday\" if x < 5 else \"weekend\")\n",
    "\n",
    "        # Classify time of day\n",
    "        conditions = [\n",
    "            (data[\"time_of_day\"] < 6 * 3600),  # Midnight to 6 AM\n",
    "            (data[\"time_of_day\"] >= 6 * 3600) & (data[\"time_of_day\"] < 12 * 3600),  # 6 AM to Noon\n",
    "            (data[\"time_of_day\"] >= 12 * 3600) & (data[\"time_of_day\"] < 18 * 3600),  # Noon to 6 PM\n",
    "            (data[\"time_of_day\"] >= 18 * 3600),  # 6 PM to Midnight\n",
    "        ]\n",
    "        choices = [\"night\", \"morning\", \"afternoon\", \"evening\"]\n",
    "        data[\"time_period\"] = np.select(conditions, choices, default=\"unknown\")\n",
    "\n",
    "        # Aggregate by weekday/weekend and time of day\n",
    "        temporal_agg = (\n",
    "            data.groupby([\"id\", \"weekday_flag\", \"time_period\"])\n",
    "            .agg({\"enmo\": \"mean\", \"light\": \"mean\", \"non-wear_flag\": \"sum\"})\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        temporal_agg.columns = [\"_\".join(col).strip() for col in temporal_agg.columns.values]\n",
    "\n",
    "        return temporal_agg.reset_index()\n",
    "\n",
    "    def process_participant_data(self, participant_id):\n",
    "        data = self.load_data(participant_id)\n",
    "        if data.empty:  # Skip processing if data is empty\n",
    "            return pd.DataFrame()\n",
    "        temporal_data = self.temporal_aggregations(data)\n",
    "        return temporal_data\n",
    "\n",
    "    def process_all_participants(self):\n",
    "        all_data = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for id_folder in os.listdir(self.root_dir):\n",
    "                if not id_folder.startswith(\"id=\"):\n",
    "                    continue\n",
    "                participant_id = id_folder.split(\"=\")[-1]\n",
    "                futures.append(executor.submit(self.process_participant_data, participant_id))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing participants\"):\n",
    "                participant_data = future.result()\n",
    "                if not participant_data.empty:  # Only append non-empty data\n",
    "                    all_data.append(participant_data)\n",
    "\n",
    "        feature_table = pd.concat(all_data, ignore_index=True)\n",
    "        return feature_table\n",
    "\n",
    "\n",
    "# Initialize with the root directory containing participant data\n",
    "temporal = ActigraphAggregation(root_dir=\"../../data/series_test.parquet/\")\n",
    "\n",
    "# Process all participants and get the final aggregated feature table\n",
    "feature_table = temporal.process_all_participants()\n",
    "\n",
    "# Inspect the feature table\n",
    "print(feature_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ActigraphAggregation:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_data(self, participant_id):\n",
    "        file_path = os.path.join(self.root_dir, f\"id={participant_id}\", \"part-0.parquet\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found for participant {participant_id}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if file doesn't exist\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data[\"id\"] = participant_id\n",
    "        print(f\"Loaded data columns for {participant_id}: {data.columns.tolist()}\")\n",
    "        return data\n",
    "\n",
    "    def aggregate_actigraphy(self, data):\n",
    "        \"\"\"\n",
    "        Aggregate the actigraphy data for each participant with summary statistics.\n",
    "\n",
    "        Parameters:\n",
    "        - data (DataFrame): Actigraphy data for a participant.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Aggregated actigraphy data with summary statistics for each `id`.\n",
    "        \"\"\"\n",
    "        aggregated_df = (\n",
    "            data.groupby(\"id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"X\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Y\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Z\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"enmo\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"anglez\": \"mean\",  # Mean of the angle metric\n",
    "                    \"non-wear_flag\": \"sum\",  # Total non-wear time\n",
    "                    \"light\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"battery_voltage\": \"mean\",  # Average battery voltage\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Flatten the multi-level column names\n",
    "        aggregated_df.columns = [\"_\".join(col).strip() if col[1] else col[0] for col in aggregated_df.columns.values]\n",
    "        return aggregated_df\n",
    "\n",
    "    def temporal_aggregations(self, data):\n",
    "        # Classify weekday/weekend\n",
    "        data[\"weekday_flag\"] = data[\"weekday\"].apply(lambda x: \"weekday\" if x < 5 else \"weekend\")\n",
    "\n",
    "        # Classify time of day\n",
    "        conditions = [\n",
    "            (data[\"time_of_day\"] < 6 * 3600),  # Midnight to 6 AM\n",
    "            (data[\"time_of_day\"] >= 6 * 3600) & (data[\"time_of_day\"] < 12 * 3600),  # 6 AM to Noon\n",
    "            (data[\"time_of_day\"] >= 12 * 3600) & (data[\"time_of_day\"] < 18 * 3600),  # Noon to 6 PM\n",
    "            (data[\"time_of_day\"] >= 18 * 3600),  # 6 PM to Midnight\n",
    "        ]\n",
    "        choices = [\"night\", \"morning\", \"afternoon\", \"evening\"]\n",
    "        data[\"time_period\"] = np.select(conditions, choices, default=\"unknown\")\n",
    "\n",
    "        # Aggregate by weekday/weekend and time of day\n",
    "        temporal_agg = (\n",
    "            data.groupby([\"id\", \"weekday_flag\", \"time_period\"])\n",
    "            .agg({\"enmo\": \"mean\", \"light\": \"mean\", \"non-wear_flag\": \"sum\"})\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        temporal_agg.columns = [\"_\".join(col).strip() for col in temporal_agg.columns.values]\n",
    "\n",
    "        return temporal_agg.reset_index()\n",
    "\n",
    "    def activity_ratios(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the ratio of non-wear time to total measurement time for each participant.\n",
    "\n",
    "        Parameters:\n",
    "        - data (DataFrame): Actigraphy data for a participant.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Contains `non_wear_ratio` for each `id`.\n",
    "        \"\"\"\n",
    "        total_time = data.groupby(\"id\").size().rename(\"total_time\")\n",
    "        non_wear_time = data.groupby(\"id\")[\"non-wear_flag\"].sum().rename(\"non_wear_time\")\n",
    "\n",
    "        # Calculate ratio\n",
    "        ratios = pd.concat([total_time, non_wear_time], axis=1)\n",
    "        ratios[\"non_wear_ratio\"] = ratios[\"non_wear_time\"] / ratios[\"total_time\"]\n",
    "\n",
    "        return ratios[[\"non_wear_ratio\"]].reset_index()\n",
    "\n",
    "    def process_participant_data(self, participant_id):\n",
    "        data = self.load_data(participant_id)\n",
    "        if data.empty:  # Skip processing if data is empty\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Calculate aggregate statistics, temporal aggregation, and activity ratios\n",
    "        aggregate_data = self.aggregate_actigraphy(data)\n",
    "        temporal_data = self.temporal_aggregations(data)\n",
    "        ratio_data = self.activity_ratios(data)\n",
    "\n",
    "        # Merge all feature data on `id`\n",
    "        participant_data = aggregate_data.merge(temporal_data, on=\"id\", how=\"left\")\n",
    "        participant_data = participant_data.merge(ratio_data, on=\"id\", how=\"left\")\n",
    "        return participant_data\n",
    "\n",
    "    def process_all_participants(self):\n",
    "        all_data = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for id_folder in os.listdir(self.root_dir):\n",
    "                if not id_folder.startswith(\"id=\"):\n",
    "                    continue\n",
    "                participant_id = id_folder.split(\"=\")[-1]\n",
    "                futures.append(executor.submit(self.process_participant_data, participant_id))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing participants\"):\n",
    "                participant_data = future.result()\n",
    "                if not participant_data.empty:  # Only append non-empty data\n",
    "                    all_data.append(participant_data)\n",
    "\n",
    "        feature_table = pd.concat(all_data, ignore_index=True)\n",
    "        return feature_table\n",
    "\n",
    "\n",
    "# Initialize with the root directory containing participant data\n",
    "aggregator = ActigraphAggregation(root_dir=\"../../data/series_test.parquet/\")\n",
    "\n",
    "# Process all participants and get the final aggregated feature table\n",
    "feature_table = aggregator.process_all_participants()\n",
    "\n",
    "# Inspect the feature table\n",
    "print(feature_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActigraphyDataProcessor:\n",
    "    def __init__(self, root_dir, feature_table_path):\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_table_path = feature_table_path\n",
    "        self.features_to_forecast = [\"enmo\", \"light\", \"battery_voltage\"]\n",
    "\n",
    "    def load_data(self, participant_id):\n",
    "        file_path = os.path.join(self.root_dir, f\"id={participant_id}\", \"part-0.parquet\")\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data[\"id\"] = participant_id\n",
    "        print(f\"Loaded data columns for {participant_id}: {data.columns.tolist()}\")\n",
    "        return data\n",
    "\n",
    "    def compute_daily_summary(self, data):\n",
    "        daily_summary = (\n",
    "            data.groupby([\"id\", \"relative_date_PCIAT\"])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"X\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "                    \"Y\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "                    \"Z\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "                    \"enmo\": [\"mean\", \"max\", \"std\"],\n",
    "                    \"anglez\": \"mean\",\n",
    "                    \"non-wear_flag\": \"sum\",\n",
    "                    \"light\": [\"mean\", \"max\", \"min\"],\n",
    "                    \"battery_voltage\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_summary.columns = [\"_\".join(col).strip() if col[1] else col[0] for col in daily_summary.columns.values]\n",
    "        print(f\"Daily summary columns after aggregation: {daily_summary.columns.tolist()}\")\n",
    "        return daily_summary\n",
    "\n",
    "    def add_temporal_features(self, daily_summary):\n",
    "        for col in [\"enmo_mean\", \"light_mean\", \"battery_voltage_mean\"]:\n",
    "            if col in daily_summary.columns:\n",
    "                daily_summary[f\"{col}_lag1\"] = daily_summary.groupby(\"id\")[col].shift(1)\n",
    "                daily_summary[f\"{col}_rolling3\"] = (\n",
    "                    daily_summary.groupby(\"id\")[col].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Warning: Column {col} not found in daily_summary.\")\n",
    "        return daily_summary\n",
    "\n",
    "    def forecast_features(self, data, participant_id):\n",
    "        date_mapping = data[[\"time_of_day\", \"relative_date_PCIAT\"]].drop_duplicates()\n",
    "        date_mapping[\"ds\"] = pd.to_datetime(date_mapping[\"time_of_day\"], unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "        predictions = []\n",
    "        for feature in self.features_to_forecast:\n",
    "            df = data[[\"time_of_day\", feature]].dropna().rename(columns={\"time_of_day\": \"ds\", feature: \"y\"})\n",
    "            max_time = df[\"ds\"].max()\n",
    "            if max_time > 10**12:\n",
    "                df[\"ds\"] = pd.to_datetime(df[\"ds\"] / 10**3, unit=\"s\", errors=\"coerce\")\n",
    "            else:\n",
    "                df[\"ds\"] = pd.to_datetime(df[\"ds\"], unit=\"s\", errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"ds\"])\n",
    "\n",
    "            model = Prophet(daily_seasonality=True, weekly_seasonality=True)\n",
    "            model.fit(df)\n",
    "            future = model.make_future_dataframe(periods=7, freq=\"D\")\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            forecast[\"id\"] = participant_id\n",
    "            forecast = forecast[[\"id\", \"ds\", \"yhat\"]].rename(columns={\"yhat\": f\"{feature}_forecast\"})\n",
    "            predictions.append(forecast)\n",
    "\n",
    "        predictions_df = pd.concat(predictions, axis=1)\n",
    "        predictions_df = predictions_df.loc[:, ~predictions_df.columns.duplicated()]\n",
    "        predictions_df = pd.merge(predictions_df, date_mapping[[\"ds\", \"relative_date_PCIAT\"]], on=\"ds\", how=\"left\")\n",
    "\n",
    "        return predictions_df\n",
    "\n",
    "    def process_participant_data(self, participant_id):\n",
    "        data = self.load_data(participant_id)\n",
    "        daily_summary = self.compute_daily_summary(data)\n",
    "        print(f\"Daily summary columns for {participant_id}: {daily_summary.columns.tolist()}\")\n",
    "        daily_summary = self.add_temporal_features(daily_summary)\n",
    "        forecasts = self.forecast_features(data, participant_id)\n",
    "        print(f\"Forecast columns for {participant_id}: {forecasts.columns.tolist()}\")\n",
    "        processed_data = pd.merge(daily_summary, forecasts, on=[\"id\", \"relative_date_PCIAT\"], how=\"left\")\n",
    "        return processed_data\n",
    "\n",
    "    def process_all_participants(self):\n",
    "        all_data = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for id_folder in os.listdir(self.root_dir):\n",
    "                if not id_folder.startswith(\"id=\"):\n",
    "                    continue\n",
    "                participant_id = id_folder.split(\"=\")[-1]\n",
    "                futures.append(executor.submit(self.process_participant_data, participant_id))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing participants\"):\n",
    "                participant_data = future.result()\n",
    "                all_data.append(participant_data)\n",
    "\n",
    "        feature_table = pd.concat(all_data, ignore_index=True)\n",
    "        return feature_table\n",
    "\n",
    "    def save_feature_table(self):\n",
    "        os.makedirs(os.path.dirname(self.feature_table_path), exist_ok=True)\n",
    "        feature_table = self.process_all_participants()\n",
    "        feature_table.to_parquet(self.feature_table_path, index=False)\n",
    "        print(f\"Feature table saved at {self.feature_table_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MultiClassLightGBM:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.model = None\n",
    "\n",
    "    def load_config(self, config_path):\n",
    "        \"\"\"Load configuration file.\"\"\"\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        return config\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"Load dataset and split into features and target based on config.\"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        X = data[self.config[\"num_features\"] + self.config[\"cat_features\"]]\n",
    "        y = data[self.config[\"target\"]]\n",
    "        return X, y\n",
    "\n",
    "    def preprocess_data(self, X):\n",
    "        \"\"\"Preprocess the data by encoding categorical variables.\"\"\"\n",
    "        X = pd.get_dummies(X, columns=self.config[\"cat_features\"], drop_first=True)\n",
    "        return X\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the LightGBM model.\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "        # LightGBM dataset\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        # Set LightGBM parameters for multiclass classification\n",
    "        params = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"num_class\": len(y.unique()),\n",
    "            \"learning_rate\": self.config[\"model_parameters\"][\"learning_rate\"],\n",
    "            \"n_estimators\": self.config[\"model_parameters\"][\"n_estimators\"],\n",
    "            \"max_depth\": self.config[\"model_parameters\"][\"max_depth\"],\n",
    "            \"metric\": \"multi_logloss\",\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        self.model = lgb.train(\n",
    "            params, train_data, valid_sets=[train_data, val_data], early_stopping_rounds=50, verbose_eval=10\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        joblib.dump(self.model, \"lgbm_model.pkl\")\n",
    "        print(\"Model training complete and saved as 'lgbm_model.pkl'\")\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "        y_pred = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
    "        y_pred = y_pred.argmax(axis=1)  # Get class with max probability\n",
    "\n",
    "        # Print classification report\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "\n",
    "    def feature_importance(self, X):\n",
    "        \"\"\"Display the feature importance.\"\"\"\n",
    "        importance = self.model.feature_importance(importance_type=\"gain\")\n",
    "        feature_names = X.columns\n",
    "        feature_importance = pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "        feature_importance = feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "        # Plotting feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(feature_importance[\"feature\"], feature_importance[\"importance\"], color=\"skyblue\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    def load_model(self, model_path=\"lgbm_model.pkl\"):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        print(\"Model loaded from\", model_path)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions with the trained model.\"\"\"\n",
    "        X_preprocessed = self.preprocess_data(X)\n",
    "        y_pred = self.model.predict(X_preprocessed, num_iteration=self.model.best_iteration)\n",
    "        return y_pred.argmax(axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration and paths\n",
    "    config_path = \"project_config.yml\"\n",
    "    train_path = \"train.csv\"\n",
    "\n",
    "    # Initialize model class\n",
    "    lgbm_classifier = MultiClassLightGBM(config_path)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    X, y = lgbm_classifier.load_data(train_path)\n",
    "    X = lgbm_classifier.preprocess_data(X)\n",
    "\n",
    "    # Train model\n",
    "    lgbm_classifier.train(X, y)\n",
    "\n",
    "    # Evaluate model\n",
    "    lgbm_classifier.evaluate(X, y)\n",
    "\n",
    "    # Display feature importance\n",
    "    lgbm_classifier.feature_importance(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    def __init__(self, train_df, config_path):\n",
    "        self.train_df = train_df\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.num_features = self.config[\"num_features\"]\n",
    "        self.cat_features = self.config[\"cat_features\"]\n",
    "        self.target = self.config[\"target\"]\n",
    "\n",
    "    def load_config(self, config_path):\n",
    "        \"\"\"Load the project configuration file.\"\"\"\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        return config\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the train dataset by handling missing values and data type conversions.\"\"\"\n",
    "        self.handle_missing_values()\n",
    "        self.convert_data_types()\n",
    "        return self.train_df\n",
    "\n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing values in the train dataset.\"\"\"\n",
    "        # Fill numeric columns with mean\n",
    "        numeric_cols = self.train_df[self.num_features]\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        self.train_df[self.num_features] = imputer.fit_transform(numeric_cols)\n",
    "\n",
    "        # Fill categorical columns with mode\n",
    "        for col in self.cat_features:\n",
    "            self.train_df[col].fillna(self.train_df[col].mode()[0], inplace=True)\n",
    "\n",
    "    def convert_data_types(self):\n",
    "        \"\"\"Convert categorical columns to appropriate data types.\"\"\"\n",
    "        # Convert 'Sex' to binary encoding if it's part of numerical features\n",
    "        if \"Basic_Demos-Sex\" in self.num_features:\n",
    "            self.train_df[\"Basic_Demos-Sex\"] = self.train_df[\"Basic_Demos-Sex\"].map({\"Male\": 1, \"Female\": 0})\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        \"\"\"Perform feature engineering to create new features.\"\"\"\n",
    "        self.add_age_groups()\n",
    "        self.one_hot_encode_seasons()\n",
    "        self.calculate_behavioral_scores()\n",
    "        self.add_interaction_features()\n",
    "        return self.train_df\n",
    "\n",
    "    def add_age_groups(self):\n",
    "        \"\"\"Add age groups based on age.\"\"\"\n",
    "        if \"Basic_Demos-Age\" in self.num_features:\n",
    "            self.train_df[\"Age_Group\"] = pd.cut(\n",
    "                self.train_df[\"Basic_Demos-Age\"], bins=[0, 12, 17, 25], labels=[\"Child\", \"Teen\", \"Young Adult\"]\n",
    "            )\n",
    "\n",
    "    def one_hot_encode_seasons(self):\n",
    "        \"\"\"One-hot encode season columns.\"\"\"\n",
    "        for col in self.cat_features:\n",
    "            if \"Season\" in col:\n",
    "                one_hot = pd.get_dummies(self.train_df[col], prefix=col)\n",
    "                self.train_df = pd.concat([self.train_df, one_hot], axis=1)\n",
    "\n",
    "    def calculate_behavioral_scores(self):\n",
    "        \"\"\"Calculate behavioral and psychological indicators.\"\"\"\n",
    "        # Bin PCIAT total score\n",
    "        if \"PCIAT-PCIAT_Total\" in self.num_features:\n",
    "            self.train_df[\"PCIAT_Bin\"] = pd.cut(\n",
    "                self.train_df[\"PCIAT-PCIAT_Total\"], bins=[0, 20, 40, 60], labels=[\"Mild\", \"Moderate\", \"Severe\"]\n",
    "            )\n",
    "\n",
    "        # Categorize internet use\n",
    "        if \"PreInt_EduHx-computerinternet_hoursday\" in self.num_features:\n",
    "            self.train_df[\"Internet_Use_Category\"] = pd.cut(\n",
    "                self.train_df[\"PreInt_EduHx-computerinternet_hoursday\"],\n",
    "                bins=[0, 1, 3, 6, np.inf],\n",
    "                labels=[\"Low\", \"Moderate\", \"High\", \"Very High\"],\n",
    "            )\n",
    "\n",
    "    def add_interaction_features(self):\n",
    "        \"\"\"Add interaction features, such as age-adjusted scores.\"\"\"\n",
    "        # Age-adjusted CGAS Score\n",
    "        if \"CGAS-CGAS_Score\" in self.num_features and \"Basic_Demos-Age\" in self.num_features:\n",
    "            self.train_df[\"Age_Adjusted_CGAS\"] = self.train_df[\"CGAS-CGAS_Score\"] / self.train_df[\"Basic_Demos-Age\"]\n",
    "\n",
    "        # BMI Categories\n",
    "        if \"Physical-BMI\" in self.num_features:\n",
    "            self.train_df[\"BMI_Category\"] = pd.cut(\n",
    "                self.train_df[\"Physical-BMI\"],\n",
    "                bins=[0, 18.5, 25, 30, np.inf],\n",
    "                labels=[\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"],\n",
    "            )\n",
    "\n",
    "    def scale_numeric_features(self):\n",
    "        \"\"\"Scale numeric features in the final dataset.\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        self.train_df[self.num_features] = scaler.fit_transform(self.train_df[self.num_features])\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Run the complete processing pipeline.\"\"\"\n",
    "        self.preprocess_data()\n",
    "        self.feature_engineering()\n",
    "        self.scale_numeric_features()\n",
    "        return self.train_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Load train data\n",
    "train_df = pd.read_csv(\"path/to/train.csv\")\n",
    "\n",
    "# Initialize and process the data using the config file\n",
    "processor = TrainDataProcessor(train_df, \"project_config.yml\")\n",
    "processed_df = processor.process()\n",
    "\n",
    "# Inspect the processed dataframe\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|██████████| 996/996 [02:09<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    X_mean     X_std     X_max     X_min    Y_mean     Y_std  \\\n",
      "0  6b6467f4 -0.425470  0.471329  1.006836 -0.992422  0.056812  0.513878   \n",
      "1  92bb8516 -0.360749  0.487472  2.691724 -2.232233 -0.065675  0.473331   \n",
      "2  92bb8516 -0.360749  0.487472  2.691724 -2.232233 -0.065675  0.473331   \n",
      "3  0d01bbf2 -0.478973  0.429476  1.159667 -3.298790 -0.037643  0.518888   \n",
      "4  0d01bbf2 -0.478973  0.429476  1.159667 -3.298790 -0.037643  0.518888   \n",
      "\n",
      "      Y_max     Y_min    Z_mean  ...  light_min  battery_voltage_mean  \\\n",
      "0  1.009336 -3.549023 -0.307925  ...        0.0           4095.802734   \n",
      "1  1.429382 -2.847736  0.091206  ...        0.0           3948.139404   \n",
      "2  1.429382 -2.847736  0.091206  ...        0.0           3948.139404   \n",
      "3  2.525316 -3.262288 -0.215956  ...        0.0           3876.515625   \n",
      "4  2.525316 -3.262288 -0.215956  ...        0.0           3876.515625   \n",
      "\n",
      "   weekday_flag  enmo_evening  enmo_night  light_evening  light_night  \\\n",
      "0       weekday      0.053129    0.016914      10.546074     8.000000   \n",
      "1       weekday      0.062285         NaN      53.124660          NaN   \n",
      "2       weekend      0.079078         NaN      84.444756          NaN   \n",
      "3       weekday      0.059008    0.000451      49.226971     0.842273   \n",
      "4       weekend      0.064596    0.123637      32.122509     1.175398   \n",
      "\n",
      "   non-wear_flag_evening  non-wear_flag_night  non_wear_ratio  \n",
      "0                    0.0                  0.0             0.0  \n",
      "1                    0.0                  NaN             0.0  \n",
      "2                    0.0                  NaN             0.0  \n",
      "3                    0.0                  0.0             0.0  \n",
      "4                    0.0                  0.0             0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ActigraphAggregation:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_data(self, participant_id):\n",
    "        file_path = os.path.join(self.root_dir, f\"id={participant_id}\", \"part-0.parquet\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found for participant {participant_id}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if file doesn't exist\n",
    "        data = pd.read_parquet(file_path)\n",
    "        data[\"id\"] = participant_id\n",
    "        # print(f\"Loaded data columns for {participant_id}: {data.columns.tolist()}\")\n",
    "        return data\n",
    "\n",
    "    def aggregate_actigraphy(self, data):\n",
    "        \"\"\"\n",
    "        Aggregate the actigraphy data for each participant with summary statistics.\n",
    "\n",
    "        Parameters:\n",
    "        - data (DataFrame): Actigraphy data for a participant.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Aggregated actigraphy data with summary statistics for each `id`.\n",
    "        \"\"\"\n",
    "        aggregated_df = (\n",
    "            data.groupby(\"id\")\n",
    "            .agg(\n",
    "                {\n",
    "                    \"X\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Y\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"Z\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"enmo\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"anglez\": \"mean\",  # Mean of the angle metric\n",
    "                    \"non-wear_flag\": \"sum\",  # Total non-wear time\n",
    "                    \"light\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "                    \"battery_voltage\": \"mean\",  # Average battery voltage\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Flatten the multi-level column names\n",
    "        aggregated_df.columns = [\"_\".join(col).strip() if col[1] else col[0] for col in aggregated_df.columns.values]\n",
    "        return aggregated_df\n",
    "\n",
    "    def temporal_aggregations(self, data):\n",
    "        # Classify weekday/weekend\n",
    "        data[\"weekday_flag\"] = data[\"weekday\"].apply(lambda x: \"weekday\" if x < 5 else \"weekend\")\n",
    "\n",
    "        # Classify time of day\n",
    "        conditions = [\n",
    "            (data[\"time_of_day\"] < 6 * 3600),  # Midnight to 6 AM\n",
    "            (data[\"time_of_day\"] >= 6 * 3600) & (data[\"time_of_day\"] < 12 * 3600),  # 6 AM to Noon\n",
    "            (data[\"time_of_day\"] >= 12 * 3600) & (data[\"time_of_day\"] < 18 * 3600),  # Noon to 6 PM\n",
    "            (data[\"time_of_day\"] >= 18 * 3600),  # 6 PM to Midnight\n",
    "        ]\n",
    "        choices = [\"night\", \"morning\", \"afternoon\", \"evening\"]\n",
    "        data[\"time_period\"] = np.select(conditions, choices, default=\"unknown\")\n",
    "\n",
    "        # Aggregate by weekday/weekend and time of day\n",
    "        temporal_agg = (\n",
    "            data.groupby([\"id\", \"weekday_flag\", \"time_period\"])\n",
    "            .agg({\"enmo\": \"mean\", \"light\": \"mean\", \"non-wear_flag\": \"sum\"})\n",
    "            .unstack(fill_value=0)\n",
    "        )\n",
    "        temporal_agg.columns = [\"_\".join(col).strip() for col in temporal_agg.columns.values]\n",
    "\n",
    "        return temporal_agg.reset_index()\n",
    "\n",
    "    def activity_ratios(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the ratio of non-wear time to total measurement time for each participant.\n",
    "\n",
    "        Parameters:\n",
    "        - data (DataFrame): Actigraphy data for a participant.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Contains `non_wear_ratio` for each `id`.\n",
    "        \"\"\"\n",
    "        total_time = data.groupby(\"id\").size().rename(\"total_time\")\n",
    "        non_wear_time = data.groupby(\"id\")[\"non-wear_flag\"].sum().rename(\"non_wear_time\")\n",
    "\n",
    "        # Calculate ratio\n",
    "        ratios = pd.concat([total_time, non_wear_time], axis=1)\n",
    "        ratios[\"non_wear_ratio\"] = ratios[\"non_wear_time\"] / ratios[\"total_time\"]\n",
    "\n",
    "        return ratios[[\"non_wear_ratio\"]].reset_index()\n",
    "\n",
    "    def process_participant_data(self, participant_id):\n",
    "        data = self.load_data(participant_id)\n",
    "        if data.empty:  # Skip processing if data is empty\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Calculate aggregate statistics, temporal aggregation, and activity ratios\n",
    "        aggregate_data = self.aggregate_actigraphy(data)\n",
    "        temporal_data = self.temporal_aggregations(data)\n",
    "        ratio_data = self.activity_ratios(data)\n",
    "\n",
    "        # Merge all feature data on `id`\n",
    "        participant_data = aggregate_data.merge(temporal_data, on=\"id\", how=\"left\")\n",
    "        participant_data = participant_data.merge(ratio_data, on=\"id\", how=\"left\")\n",
    "        return participant_data\n",
    "\n",
    "    def process_all_participants(self):\n",
    "        all_data = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for id_folder in os.listdir(self.root_dir):\n",
    "                if not id_folder.startswith(\"id=\"):\n",
    "                    continue\n",
    "                participant_id = id_folder.split(\"=\")[-1]\n",
    "                futures.append(executor.submit(self.process_participant_data, participant_id))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing participants\"):\n",
    "                participant_data = future.result()\n",
    "                if not participant_data.empty:  # Only append non-empty data\n",
    "                    all_data.append(participant_data)\n",
    "\n",
    "        feature_table = pd.concat(all_data, ignore_index=True)\n",
    "        return feature_table\n",
    "\n",
    "\n",
    "# Initialize with the root directory containing participant data\n",
    "aggregator = ActigraphAggregation(root_dir=\"../../data/series_train.parquet/\")\n",
    "\n",
    "# Process all participants and get the final aggregated feature table\n",
    "feature_table = aggregator.process_all_participants()\n",
    "\n",
    "# Inspect the feature table\n",
    "print(feature_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'X_mean', 'X_std', 'X_max', 'X_min', 'Y_mean', 'Y_std', 'Y_max',\n",
       "       'Y_min', 'Z_mean', 'Z_std', 'Z_max', 'Z_min', 'enmo_mean', 'enmo_std',\n",
       "       'enmo_max', 'enmo_min', 'anglez_mean', 'non-wear_flag_sum',\n",
       "       'light_mean', 'light_std', 'light_max', 'light_min',\n",
       "       'battery_voltage_mean', 'weekday_flag', 'enmo_evening', 'enmo_night',\n",
       "       'light_evening', 'light_night', 'non-wear_flag_evening',\n",
       "       'non-wear_flag_night', 'non_wear_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train csv with feature table\n",
    "df_train = pd.read_csv(\"../../data/childhealth.csv\")\n",
    "\n",
    "df_merge = df_train.merge(feature_table, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4940, 113)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3960, 82)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1976, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ChildHealthModel:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.model = None\n",
    "\n",
    "    def load_config(self, config_path):\n",
    "        \"\"\"Load configuration file.\"\"\"\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        return config\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"Load dataset and split into features and target based on config.\"\"\"\n",
    "        data = pd.read_csv(filepath)\n",
    "        X = data[self.config[\"num_features\"] + self.config[\"cat_features\"]]\n",
    "        y = data[self.config[\"target\"]]\n",
    "        return X, y\n",
    "\n",
    "    def preprocess_data(self, X):\n",
    "        \"\"\"Preprocess the data by encoding categorical variables.\"\"\"\n",
    "        X = pd.get_dummies(X, columns=self.config[\"cat_features\"], drop_first=True)\n",
    "        return X\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the LightGBM model.\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "        # LightGBM dataset\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        # Set LightGBM parameters for multiclass classification\n",
    "        params = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"num_class\": len(y.unique()),\n",
    "            \"learning_rate\": self.config[\"model_parameters\"][\"learning_rate\"],\n",
    "            \"n_estimators\": self.config[\"model_parameters\"][\"n_estimators\"],\n",
    "            \"max_depth\": self.config[\"model_parameters\"][\"max_depth\"],\n",
    "            \"metric\": \"multi_logloss\",\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        self.model = lgb.train(\n",
    "            params, train_data, valid_sets=[train_data, val_data], early_stopping_rounds=50, verbose_eval=10\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        joblib.dump(self.model, \"lgbm_model.pkl\")\n",
    "        print(\"Model training complete and saved as 'lgbm_model.pkl'\")\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "        y_pred = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
    "        y_pred = y_pred.argmax(axis=1)  # Get class with max probability\n",
    "\n",
    "        # Print classification report\n",
    "        print(\"Evaluation Results:\")\n",
    "        print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "\n",
    "    def feature_importance(self, X):\n",
    "        \"\"\"Display the feature importance.\"\"\"\n",
    "        importance = self.model.feature_importance(importance_type=\"gain\")\n",
    "        feature_names = X.columns\n",
    "        feature_importance = pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "        feature_importance = feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "        # Plotting feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(feature_importance[\"feature\"], feature_importance[\"importance\"], color=\"skyblue\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "    def load_model(self, model_path=\"lgbm_model.pkl\"):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        print(\"Model loaded from\", model_path)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions with the trained model.\"\"\"\n",
    "        X_preprocessed = self.preprocess_data(X)\n",
    "        y_pred = self.model.predict(X_preprocessed, num_iteration=self.model.best_iteration)\n",
    "        return y_pred.argmax(axis=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration and paths\n",
    "    config_path = \"project_config.yml\"\n",
    "    train_path = \"train.csv\"\n",
    "\n",
    "    # Initialize model class\n",
    "    child_health_model = ChildHealthModel(config_path)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    X, y = child_health_model.load_data(train_path)\n",
    "    X = child_health_model.preprocess_data(X)\n",
    "\n",
    "    # Train model\n",
    "    child_health_model.train(X, y)\n",
    "\n",
    "    # Evaluate model\n",
    "    child_health_model.evaluate(X, y)\n",
    "\n",
    "    # Display feature importance\n",
    "    child_health_model.feature_importance(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
